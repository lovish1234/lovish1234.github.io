<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Lovish Chum</title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="Lovish Chum" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="stylesheet" type="text/css" href="/academicons.min.css">
  <link rel="canonical" href="http://localhost:4000/404.html">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
  

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-30336262-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-30336262-1');
  </script>
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Lovish Chum 
                <div style="line-height:0px;">
                    <br>
                </div>
                <a href="https://mailhide.io/e/pwlslZiG" onclick="popup=window.open('https://mailhide.io/e/pwlslZiG','mailhidepopup','width=580,height=635'); return false;"><tt>l......@columbia.edu</tt></a>
              </h1>
              <p> I am an <em>Applied Scientist II</em>  at <a href="https://aws.amazon.com/">Amazon Web Services (AWS) - AI</a>  where I am the part of <a href="https://aws.amazon.com/">Rekognition</a> team. My interests lie on the intersection of computer vision and natural language processing (NLP). 
              </p>

              <p> Earlier I was a Research Asosicate at Dept. of Electrical Engineering, Columbia University with <a href="https://www.ee.columbia.edu/~sfchang/">Prof. Shih-Fu Chang</a>. I received MS(by research/thesis track) in <a href="https://www.cs.columbia.edu/">Computer Science</a> from  <a href="https://www.engineering.columbia.edu/about">Fu Foundation School of Engineering</a> at <a href="https://www.columbia.edu/">Columbia University</a>, where I worked on future scene graph prediction with <a href="http://www.cs.columbia.edu/~vondrick/">Carl Vondrick</a>. 
              </p>
              <p>
                From 2017 to 2019, I worked at <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a>, as part of <a href="https://cvit.iiit.ac.in/">CVIT</a> where I worked with <a href="https://faculty.iiit.ac.in/~jawahar/">CV Jawahar</a> and <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubhramanian</a>.
                Even earlier, I was an undergrad at <a href="https://www.iitk.ac.in/">IIT Kanpur</a>.
              </p>
              <p>
                I am always looking for opportunities. Welcome to contact via e-mail if you think it can be a good fit. 
              </p>
<!--                 I have an MS in Computer Science (AI focus) from <a href="http://cs.stanford.edu">Stanford University</a>, where I was a research assistant for <a href="http://svl.stanford.edu/">Silvio Savarese</a> and a teaching assistant for <a href="http://svl.stanford.edu/">Fei-Fei Li</a> (<a href="http://vision.stanford.edu/teaching/cs131_fall1617/">CS131</a> & <a href="http://cs231n.stanford.edu/2017/">CS231N</a>). I have a BS in EECS from <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I worked in <a href="https://people.eecs.berkeley.edu/~pister/">Kris Pister's</a> lab. -->
              </p>
              <p style="text-align:center">

<!--                 <a target="_blank" href="https://mailhide.io/e/pwlslZiG"> Email</a> &nbsp;/&nbsp;
 --><!--                 <a href="https://github.com/williamd4112" target="_blank">X<i class="fab fa-github ai-2x"></i></a>&nbsp; -->

                <a href="https://github.com/lovish1234">GitHub</a> &nbsp;/&nbsp;
<!--                 <a href="https://scholar.google.com/citations?user=GCYG4GoAAAAJ">Google Scholar</a> &nbsp;/&nbsp; -->

                <a href="https://www.linkedin.com/in/lovish1234"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png">
              <center></center>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>News</h2>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <ul>
                <li>Mar 2023: <a href="https://assets.amazon.science/4a/3c/b5bcb0dc4626a016a79238d693b3/multi-scale-compositional-constraints-for-representation-learning-on-videos.pdf">Paper</a>  on video representation learning accepted at <a href="https://2023.ieeeicassp.org/">ICASSP 2023</a>  </li>
                <li>Jun 2022: <a href="https://arxiv.org/pdf/2206.07207.pdf">Paper</a> on multi-modal event graphs available on arXiV </li>
                <li>Jan 2022: Joined Amazon Web Services (AWS) - AI as an Applied Scientist </li>
                <li>Mar 2021: <a href="https://arxiv.org/abs/2109.12776">Paper</a> accepted at EMNLP Findings 2021</li>
                <li>Mar 2021: Joined <a href="https://www.ee.columbia.edu/ln/dvmm/">DVMM</a>, Columbia University as Staff Associate I. Working with Prof. Shih-Fu Chang</li>
                <li>Oct 2020: Talk on <a href="/pdfs/talks/SGG.pdf">Scene Graph Generation</a> at CV Lab, Columbia</li>
                <li>Sep 2019: Joining Columbia University as a Masters of Computer Science student in Fall 2019 </li>
                <li>Jul 2019: Talk on <a href="/pdfs/talks/LA_mlss.pdf">Linear Algebra for Machine Learning</a> at IIIT-H ML Summer School </li>
                <li>May 2019: <a href="/pdfs/papers/JIISc.pdf">Survey paper</a> accepted in Journal of Indian Institute of Science </li>
                <li>Sep 2018: Attending <a href="http://mlss.cc/" > Machine Learning Summer School 2018</a> in Madrid </li>
              </ul>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Recent Projects/Publications</h2>
              <p>
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/vrl.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World</h3>
              <br>
              Georgios Paraskevopoulos, Chandrashekhar Lavania, <strong> Lovish Chum </strong>, Shiva Sundaram

              <br>
              <em>ICASSP</em>, 2023
              <br>
              
              [<strong><a href="/pdfs/projects/ICASSP_2023.pdf">Publication</a></strong>]
              
              
              
              
              
              
              
              <p></p>
              <p>Combining simple concepts to form structured thoughts and decomposing complex concepts into their constituents is one key characteristic of human cognition. In this work, we extract video representations by combining multi-scale processing with compositional constraints, i.e., we constrain the latent space created by the network so that coarse grained video features are composed from a set of fine-grained video features using simple functions. We integrate the proposed constraints in a state-of-the-art contrastive learning framework. In our ablations, we evaluate different formulations of the compositional constraints and composition functions. We evaluate the proposed approach for the downstream tasks of action detection in UCF-101, and video summarization in the SumMe dataset. We achieve significant improvements over the baseline, i.e., 3.9% and 6.3% relative improvements for UCF-101 and SumMe respectively, showcasing the importance of compositional video representations.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/mmeg.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World</h3>
              <br>
              Hammad A. Ayyubi, Christopher Thomas, <strong>Lovish Chum</strong>, Rahul Lokesh, Yulei Niu, Xudong Lin, Long Chen, Jaywon Koo, Sounak Ray, and Shih-Fu Chang

              <br>
              <em>arXiv</em>, 2022
              <br>
              
              [<strong><a href="/pdfs/projects/arXiv_2022.pdf">Publication</a></strong>]
              
              
              
              
              
              
              
              <p></p>
              <p>Understanding how events described or shown in multimedia content relate to one another is a critical component to developing robust artificially intelligent systems which can reason about real-world media. While much research has been devoted to event understanding in the text, image, and video domains, none have explored the complex relations that events experience across domains. For example, a news article may describe a “protest” event while a video shows an “arrest” event. Recognizing that the visual “arrest” event is a subevent of the broader “protest” event is a challenging, yet important problem that prior work has not explored. In this paper, we propose the novel task of MultiModal Event Event Relations to recognize such cross-modal event relations. We contribute a large-scale dataset consisting of 100k video-news article pairs, as well as a benchmark of densely annotated data. We also propose a weakly supervised multimodal method which integrates commonsense knowledge from an external knowledge base (KB) to predict rich multimodal event hierarchies. Experiments show that our model outperforms a number of competitive baselines on our proposed benchmark. We also perform a detailed analysis of our model’s performance and suggest directions for future research.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/jmm.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Joint Multimedia Event Extraction from Video and Article</h3>
              <br>
              Brian Chen, Xudong Lin, Christopher Thomas, Manling Li, Shoya Yoshida, <strong>Lovish Chum</strong>, Heng Ji, Shih-Fu Chang

              <br>
              <em>EMNLP Findings</em>, 2021
              <br>
              
              [<strong><a href="/pdfs/projects/EMNLP_2022.pdf">Publication</a></strong>]
              
              
              
              
              
              
              
              <p></p>
              <p>Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from video and text articles. We introduce the new task of Video MultiMedia Event Extraction (Video M2E2) and propose two novel components to build the first system towards this task. First, we propose the first self-supervised multimodal event coreference model that can determine coreference between video events and text events without any manually annotated pairs. Second, we introduce the first multimodal transformer which extracts structured event information jointly from both videos and text documents. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. Our experimental results demonstrate the effectiveness of our proposed method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score gain on multimodal event coreference resolution and multimedia event extraction.</p>

            </td>
          </tr>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
        </table>
        <br>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Earlier Projects</h2>
              <p>
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/uncertain.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Encoding Unertainity in Video Representation Learning</h3>
              <br>
<!--               <em>earlier </em>
              <br> -->
              <em>Independent Study</em>, 2020
              <br>
              
              
              
<!--               
              
               -->
              
              
              
              
              
              <p></p>
              <p>Can stochasticity of future in videos help learn better representations ? To answer this question, we relax the point-based contrastive loss to a n-shpere based contrastive loss. We build our experiments on top of Dense Predictive Coding, a self-supervised representation technique.  Training on Kinetics-400 and fine-tuning on UCF-101, HMDB-51 help us understand that the later two datasets do not contain videos with diverse futures. Hence, we build a <em>block toy</em> dataset to control of stochasticity and evaluate the technique.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/causal.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Causal Physical Reasoning</h3>
              <br>
<!--               <em>earlier </em>
              <br> -->
              <em>Causal Inference Course</em>, 2020
              <br>
              
              
              
              [<strong><a href="">Report</a></strong>]              
              
<!--               
              
               -->
              
              
              
              <a href="https://github.com/lovish1234/CausalComputerVision">code</a> /
              
              
              
              <p></p>
              <p>In this project, we take a step towards learning causal visual features. For this, we have created a simulation environment called <em>Causal-PHYRE</em>. For each task in the dataset, we control the outcome, cause and confounder. We start out by observing if a vanilla NN architecture detect the causal structure of tasks from simulated videos. Further, we simulate interventions in the environment to explicitly aid learning causal structure of underlying tasks. Experiments show that intervention-based learning strategy not only improves the detection of <em>real</em> cause rather than confounder. This concurres with assertion in Pearl Causal Hierarchy (PCH), which says that it is impossible to learn causal structure using just interventional data.</p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/ODP_depth.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Optional Depth Pathway for Mask R-CNN</h3>
              <br>
<!--               <em>earlier </em>
              <br> -->
              <em>Robot Learning Course</em>, 2019
              <br>
              
              
              
              [<strong><a href="">Report</a></strong>]              
              
<!--               
              
               -->
              
              
              
              <a href="https://github.com/lovish1234/ODP">code</a> /
              
              
              
              <p></p>
              <p>Instance segmentation is one of the most important perception tasks in computer vision. We present an approach to <strong>optionally</strong> use the depth given along an image to aid the performance on this task. We observe that depth information incorporated through Spatially-Adaptive (DE)normalization (SPADE) results in significant improvement on the task on NYUv2 dataset. Additionally, we observe that the use of ODM (Optional Depth Module) helps to prevent the degradation of performance even when the depth data is unavailable to the network.</p>

            </td>
          </tr>
          
          
          
          
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/new_projects/survey_2.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Beyond Supervised Learning: A Computer Vision Perspective</h3>
              <br>
<!--               <em>earlier </em>
              <br> -->
              <em>Journal of Indian Institute of Science</em>, 2019
              <br>
              
              
              [<strong><a href="/pdfs/papers/JIISc.pdf">Publication</a></strong>]              
              
              
<!--               
              
               -->
              
              
              
              
              
              <p></p>
              <p>Fully supervised deep learning-based methods have created a profound impact in various fields of computer science. Compared to classical methods, supervised deep learning-based techniques face scalability issues as they require huge amounts of labeled data and, more significantly, are unable to generalize to multiple domains and tasks. In recent years, a lot of research has been targeted towards addressing these issues within the deep learning community. Although there have been extensive surveys on learning paradigms such as semi-supervised and unsupervised learning, there are few timely reviews after the emergence of deep learning. In this paper, we provide an overview of the contemporary literature surrounding alternatives to fully supervised learning in the deep learning context. First, we summarize the relevant techniques that fall between the paradigm of supervised and unsupervised learning. Second, we take autonomous navigation as a running example to explain and compare different models. Finally, we highlight some shortcomings of current methods and suggest future directions.</p>

            </td>
          </tr>
          
          
          
          
          
          
          
          
          
        </table>
        <br>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Teaching</h2>
              <p> I have a keen interest in <strong>community learning</strong>. While at IIIT-H, I actively took initiatives to share what I learn.  I totally believe in the following adage :
                <br>
                <br>
                <center><q>
                <p style="font-style: italic;">
                If you want to learn something, <br>
                      read about it. <br>
                If you want to understand something, <br>
                      write about it. <br>
                If you want to master something, <br>
                      teach it </q> <br>
                </p>
                </center>         
         </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/teaching/bayesian_ml.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Bayesian Machine Learning</h3>
              <br>
              <em>CVIT, IIIT Hyderabad</em>
              <br>
              2019-07-01
              <br>
              
              
<!--               
              
               -->
              
              
              
              
              
              [<strong><a href="https://drive.google.com/drive/folders/1ems060SdddeaL_HBF5ergd1ZV08ThlSi?usp=sharing">Slides</a></strong>]
              
              <p></p>
              <p>A short course on Bayesian Machine Learning covering EM, Variational Inference and Latent Variable Models in a succinct manner.</p>

            </td>
          </tr>
          
          
          
          
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:32%;vertical-align:middle;min-width:120px">
              <img src="/images/teaching/linear.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:82%;vertical-align:middle">
              <h3>Linear Algebra</h3>
              <br>
              <em>CVIT, IIIT Hyderabad</em>
              <br>
              2018-07-01
              <br>
              
              
<!--               
              
               -->
              
              
              
              
              
              [<strong><a href="https://drive.google.com/drive/folders/1DALSyj5UKWXKG3_WDkVLTw2tcIrQVuXz?usp=sharing">Slides</a></strong>]
              
              <p></p>
              <p>A short course on Linear Algebra covering matrix transformations and some form of matrix decompositions.</p>

            </td>
          </tr>
          
          
          
          
          
        </table>

        <div class="section">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:xx-small;">
                Thanks <a style="font-size:xx-small;" href="https://jonbarron.info">Jon Barron</a> for source
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>


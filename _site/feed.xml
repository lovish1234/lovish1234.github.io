<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-02T22:16:30+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lovish Chum</title><entry><title type="html">Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World" /><published>2023-03-18T00:00:00+00:00</published><updated>2023-03-18T00:00:00+00:00</updated><id>http://localhost:4000/vrl</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;Combining simple concepts to form structured thoughts and decomposing complex concepts into their constituents is one key characteristic of human cognition. In this work, we extract video representations by combining multi-scale processing with compositional constraints, i.e., we constrain the latent space created by the network so that coarse grained video features are composed from a set of fine-grained video features using simple functions. We integrate the proposed constraints in a state-of-the-art contrastive learning framework. In our ablations, we evaluate different formulations of the compositional constraints and composition functions. We evaluate the proposed approach for the downstream tasks of action detection in UCF-101, and video summarization in the SumMe dataset. We achieve significant improvements over the baseline, i.e., 3.9% and 6.3% relative improvements for UCF-101 and SumMe respectively, showcasing the importance of compositional video representations.&lt;/p&gt;</content><author><name></name></author><category term="research" /><summary type="html">Combining simple concepts to form structured thoughts and decomposing complex concepts into their constituents is one key characteristic of human cognition. In this work, we extract video representations by combining multi-scale processing with compositional constraints, i.e., we constrain the latent space created by the network so that coarse grained video features are composed from a set of fine-grained video features using simple functions. We integrate the proposed constraints in a state-of-the-art contrastive learning framework. In our ablations, we evaluate different formulations of the compositional constraints and composition functions. We evaluate the proposed approach for the downstream tasks of action detection in UCF-101, and video summarization in the SumMe dataset. We achieve significant improvements over the baseline, i.e., 3.9% and 6.3% relative improvements for UCF-101 and SumMe respectively, showcasing the importance of compositional video representations.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/vrl.png" /><media:content medium="image" url="http://localhost:4000/images/new_projects/vrl.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Multimodal Event Graphs: Towards Event Centric Understanding of Multimodal World" /><published>2022-06-14T00:00:00+00:00</published><updated>2022-06-14T00:00:00+00:00</updated><id>http://localhost:4000/mmeg</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;Understanding how events described or shown in multimedia content relate to one another is a critical component to developing robust artificially intelligent systems which can reason about real-world media. While much research has been devoted to event understanding in the text, image, and video domains, none have explored the complex relations that events experience across domains. For example, a news article may describe a “protest” event while a video shows an “arrest” event. Recognizing that the visual “arrest” event is a subevent of the broader “protest” event is a challenging, yet important problem that prior work has not explored. In this paper, we propose the novel task of MultiModal Event Event Relations to recognize such cross-modal event relations. We contribute a large-scale dataset consisting of 100k video-news article pairs, as well as a benchmark of densely annotated data. We also propose a weakly supervised multimodal method which integrates commonsense knowledge from an external knowledge base (KB) to predict rich multimodal event hierarchies. Experiments show that our model outperforms a number of competitive baselines on our proposed benchmark. We also perform a detailed analysis of our model’s performance and suggest directions for future research.&lt;/p&gt;</content><author><name></name></author><category term="research" /><summary type="html">Understanding how events described or shown in multimedia content relate to one another is a critical component to developing robust artificially intelligent systems which can reason about real-world media. While much research has been devoted to event understanding in the text, image, and video domains, none have explored the complex relations that events experience across domains. For example, a news article may describe a “protest” event while a video shows an “arrest” event. Recognizing that the visual “arrest” event is a subevent of the broader “protest” event is a challenging, yet important problem that prior work has not explored. In this paper, we propose the novel task of MultiModal Event Event Relations to recognize such cross-modal event relations. We contribute a large-scale dataset consisting of 100k video-news article pairs, as well as a benchmark of densely annotated data. We also propose a weakly supervised multimodal method which integrates commonsense knowledge from an external knowledge base (KB) to predict rich multimodal event hierarchies. Experiments show that our model outperforms a number of competitive baselines on our proposed benchmark. We also perform a detailed analysis of our model’s performance and suggest directions for future research.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/mmeg.png" /><media:content medium="image" url="http://localhost:4000/images/new_projects/mmeg.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Joint Multimedia Event Extraction from Video and Article</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Joint Multimedia Event Extraction from Video and Article" /><published>2021-09-27T00:00:00+00:00</published><updated>2021-09-27T00:00:00+00:00</updated><id>http://localhost:4000/jmm</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from video and text articles. We introduce the new task of Video MultiMedia Event Extraction (Video M2E2) and propose two novel components to build the first system towards this task. First, we propose the first self-supervised multimodal event coreference model that can determine coreference between video events and text events without any manually annotated pairs. Second, we introduce the first multimodal transformer which extracts structured event information jointly from both videos and text documents. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. Our experimental results demonstrate the effectiveness of our proposed method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score gain on multimodal event coreference resolution and multimedia event extraction.&lt;/p&gt;</content><author><name></name></author><category term="research" /><summary type="html">Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from video and text articles. We introduce the new task of Video MultiMedia Event Extraction (Video M2E2) and propose two novel components to build the first system towards this task. First, we propose the first self-supervised multimodal event coreference model that can determine coreference between video events and text events without any manually annotated pairs. Second, we introduce the first multimodal transformer which extracts structured event information jointly from both videos and text documents. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. Our experimental results demonstrate the effectiveness of our proposed method on our new benchmark dataset. We achieve 6.0% and 5.8% absolute F-score gain on multimodal event coreference resolution and multimedia event extraction.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/jmm.png" /><media:content medium="image" url="http://localhost:4000/images/new_projects/jmm.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Sgg</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Sgg" /><published>2020-11-22T00:00:00+00:00</published><updated>2020-11-22T00:00:00+00:00</updated><id>http://localhost:4000/sgg</id><content type="html" xml:base="http://localhost:4000/">&lt;!-- ---
layout: post
title:  &quot;Spatio-temporal Scene Graph Generation (Ongoing)&quot;
categories: earlier
authors: &lt;strong&gt;Lovish Chum&lt;/strong&gt;, with Xudong Lin and Carl Vondrick
image: /images/new_projects/sgg.png
venue: Part of MS thesis
--- --&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Encoding Unertainity in Video Representation Learning</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Encoding Unertainity in Video Representation Learning" /><published>2020-06-15T00:00:00+00:00</published><updated>2020-06-15T00:00:00+00:00</updated><id>http://localhost:4000/dpc</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;Can stochasticity of future in videos help learn better representations ? To answer this question, we relax the point-based contrastive loss to a n-shpere based contrastive loss. We build our experiments on top of Dense Predictive Coding, a self-supervised representation technique.  Training on Kinetics-400 and fine-tuning on UCF-101, HMDB-51 help us understand that the later two datasets do not contain videos with diverse futures. Hence, we build a &lt;em&gt;block toy&lt;/em&gt; dataset to control of stochasticity and evaluate the technique.&lt;/p&gt;</content><author><name></name></author><category term="earlier" /><summary type="html">Can stochasticity of future in videos help learn better representations ? To answer this question, we relax the point-based contrastive loss to a n-shpere based contrastive loss. We build our experiments on top of Dense Predictive Coding, a self-supervised representation technique. Training on Kinetics-400 and fine-tuning on UCF-101, HMDB-51 help us understand that the later two datasets do not contain videos with diverse futures. Hence, we build a block toy dataset to control of stochasticity and evaluate the technique.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/uncertain.png" /><media:content medium="image" url="http://localhost:4000/images/new_projects/uncertain.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Causal Physical Reasoning</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Causal Physical Reasoning" /><published>2020-05-15T00:00:00+00:00</published><updated>2020-05-15T00:00:00+00:00</updated><id>http://localhost:4000/causal-vision</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;In this project, we take a step towards learning causal visual features. For this, we have created a simulation environment called &lt;em&gt;Causal-PHYRE&lt;/em&gt;. For each task in the dataset, we control the outcome, cause and confounder. We start out by observing if a vanilla NN architecture detect the causal structure of tasks from simulated videos. Further, we simulate interventions in the environment to explicitly aid learning causal structure of underlying tasks. Experiments show that intervention-based learning strategy not only improves the detection of &lt;em&gt;real&lt;/em&gt; cause rather than confounder. This concurres with assertion in Pearl Causal Hierarchy (PCH), which says that it is impossible to learn causal structure using just interventional data.&lt;/p&gt;</content><author><name></name></author><category term="earlier" /><summary type="html">In this project, we take a step towards learning causal visual features. For this, we have created a simulation environment called Causal-PHYRE. For each task in the dataset, we control the outcome, cause and confounder. We start out by observing if a vanilla NN architecture detect the causal structure of tasks from simulated videos. Further, we simulate interventions in the environment to explicitly aid learning causal structure of underlying tasks. Experiments show that intervention-based learning strategy not only improves the detection of real cause rather than confounder. This concurres with assertion in Pearl Causal Hierarchy (PCH), which says that it is impossible to learn causal structure using just interventional data.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/causal.gif" /><media:content medium="image" url="http://localhost:4000/images/new_projects/causal.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Optional Depth Pathway for Mask R-CNN</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Optional Depth Pathway for Mask R-CNN" /><published>2019-12-22T00:00:00+00:00</published><updated>2019-12-22T00:00:00+00:00</updated><id>http://localhost:4000/odp</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;Instance segmentation is one of the most important perception tasks in computer vision. We present an approach to &lt;strong&gt;optionally&lt;/strong&gt; use the depth given along an image to aid the performance on this task. We observe that depth information incorporated through Spatially-Adaptive (DE)normalization (SPADE) results in significant improvement on the task on NYUv2 dataset. Additionally, we observe that the use of ODM (Optional Depth Module) helps to prevent the degradation of performance even when the depth data is unavailable to the network.&lt;/p&gt;

&lt;!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ --&gt;</content><author><name></name></author><category term="earlier" /><summary type="html">Instance segmentation is one of the most important perception tasks in computer vision. We present an approach to optionally use the depth given along an image to aid the performance on this task. We observe that depth information incorporated through Spatially-Adaptive (DE)normalization (SPADE) results in significant improvement on the task on NYUv2 dataset. Additionally, we observe that the use of ODM (Optional Depth Module) helps to prevent the degradation of performance even when the depth data is unavailable to the network.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/ODP_depth.gif" /><media:content medium="image" url="http://localhost:4000/images/new_projects/ODP_depth.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bayesian Machine Learning</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Bayesian Machine Learning" /><published>2019-07-01T00:00:00+00:00</published><updated>2019-07-01T00:00:00+00:00</updated><id>http://localhost:4000/bayesian</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;A short course on Bayesian Machine Learning covering EM, Variational Inference and Latent Variable Models in a succinct manner.&lt;/p&gt;</content><author><name></name></author><category term="teaching" /><summary type="html">A short course on Bayesian Machine Learning covering EM, Variational Inference and Latent Variable Models in a succinct manner.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/teaching/bayesian_ml.png" /><media:content medium="image" url="http://localhost:4000/images/teaching/bayesian_ml.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Beyond Supervised Learning: A Computer Vision Perspective</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Beyond Supervised Learning: A Computer Vision Perspective" /><published>2019-04-30T00:00:00+00:00</published><updated>2019-04-30T00:00:00+00:00</updated><id>http://localhost:4000/survey</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;Fully supervised deep learning-based methods have created a profound impact in various fields of computer science. Compared to classical methods, supervised deep learning-based techniques face scalability issues as they require huge amounts of labeled data and, more significantly, are unable to generalize to multiple domains and tasks. In recent years, a lot of research has been targeted towards addressing these issues within the deep learning community. Although there have been extensive surveys on learning paradigms such as semi-supervised and unsupervised learning, there are few timely reviews after the emergence of deep learning. In this paper, we provide an overview of the contemporary literature surrounding alternatives to fully supervised learning in the deep learning context. First, we summarize the relevant techniques that fall between the paradigm of supervised and unsupervised learning. Second, we take autonomous navigation as a running example to explain and compare different models. Finally, we highlight some shortcomings of current methods and suggest future directions.&lt;/p&gt;</content><author><name></name></author><category term="earlier" /><summary type="html">Fully supervised deep learning-based methods have created a profound impact in various fields of computer science. Compared to classical methods, supervised deep learning-based techniques face scalability issues as they require huge amounts of labeled data and, more significantly, are unable to generalize to multiple domains and tasks. In recent years, a lot of research has been targeted towards addressing these issues within the deep learning community. Although there have been extensive surveys on learning paradigms such as semi-supervised and unsupervised learning, there are few timely reviews after the emergence of deep learning. In this paper, we provide an overview of the contemporary literature surrounding alternatives to fully supervised learning in the deep learning context. First, we summarize the relevant techniques that fall between the paradigm of supervised and unsupervised learning. Second, we take autonomous navigation as a running example to explain and compare different models. Finally, we highlight some shortcomings of current methods and suggest future directions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/new_projects/survey_2.png" /><media:content medium="image" url="http://localhost:4000/images/new_projects/survey_2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Linear Algebra</title><link href="http://localhost:4000/" rel="alternate" type="text/html" title="Linear Algebra" /><published>2018-07-01T00:00:00+00:00</published><updated>2018-07-01T00:00:00+00:00</updated><id>http://localhost:4000/linear</id><content type="html" xml:base="http://localhost:4000/">&lt;p&gt;A short course on Linear Algebra covering matrix transformations and some form of matrix decompositions.&lt;/p&gt;</content><author><name></name></author><category term="teaching" /><summary type="html">A short course on Linear Algebra covering matrix transformations and some form of matrix decompositions.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/teaching/linear.png" /><media:content medium="image" url="http://localhost:4000/images/teaching/linear.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>